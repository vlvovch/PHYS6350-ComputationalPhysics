{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Lecture Materials\n",
    ":class: tip\n",
    "[Download the slide deck for this lecture](/_static/pdf/Lecture7-NonlinearEquations-2.pdf)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function minimization\n",
    "\n",
    "Function minimization is one of the most common tasks in physics and engineering. It is used in many different contexts, such as energy minimization in statistical physics, optimization of parameters in model fitting or machine learning, etc.\n",
    "\n",
    "Function minimization has a close relationship with the root finding problem. \n",
    "In particular, it can be reduced to finding the root of the first derivative of the function.\n",
    "\n",
    "Let us consider different methods for function minimization. \n",
    "As an example, let us consider the function $f(x) = \\sin(x)$ on an interval $[0, 2\\pi]$.\n",
    "\n",
    "![sinx-minimum](sinx-minimum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden Section Search\n",
    "\n",
    "The Golden Section Search is an efficient technique for finding the minimum (or maximum) of a unimodal function (has only one minimum in the interval) within a specified interval without requiring derivatives. \n",
    "This method resembles the bisection method for root-finding in that it also decreases the interval at each step by a specified factor. \n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Bracket the minimum $x_{min}$ in interval $(a,b)$\n",
    "2. Take $c = b - (b-a)/\\varphi$ and $d = a + (b-a)/\\varphi$\n",
    "3. If $f(c) < f(d)$, take $b = d$ as new right endpoint\n",
    "4. Otherwise, take $a = c$ as new left endpoint\n",
    "5. Repeat over the new, smaller interval $(a,b)$ until the desired accuracy is reached\n",
    "\n",
    "Where $\\varphi = \\frac{1 + \\sqrt{5}}{2} = 1.618...$ is the **golden ratio**\n",
    "\n",
    "### Properties\n",
    "\n",
    "- This value ensures that the interval containing the minimum decreases by factor $\\varphi$ in each iteration\n",
    "- The method is guaranteed to work when the function is continuous and **unimodal** (has only one minimum in the interval)\n",
    "- No derivatives are required, making it suitable for functions where derivatives are difficult to compute\n",
    "\n",
    "\n",
    "The points $c$ and $d$ divide the interval $(a,b)$ according to the golden ratio, with the larger portion adjacent to the current endpoints. This placement ensures that after each comparison, we can reuse one of the previously computed function values, requiring only one new function evaluation per iteration.\n",
    "\n",
    "\n",
    "The golden ratio provides the optimal reduction factor for this type of search. Using any other ratio would require more function evaluations to achieve the same accuracy. One variation of the method often used in computer science is the [ternary search](https://en.wikipedia.org/wiki/Ternary_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "phi = (math.sqrt(5) + 1) / 2\n",
    "\n",
    "# Iterative implementation\n",
    "def gss(f, a, b, accuracy=1e-7):\n",
    "    c = b - (b - a) / phi\n",
    "    d = a + (b - a) / phi\n",
    "    while abs(b - a) > accuracy:\n",
    "        if f(c) < f(d): \n",
    "            b = d\n",
    "        else:\n",
    "            a = c\n",
    "\n",
    "        c = b - (b - a) / phi\n",
    "        d = a + (b - a) / phi\n",
    "\n",
    "    return (b + a) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive implementation\n",
    "def gss_recursive(f, a, b, accuracy=1e-7):\n",
    "    if (abs(b - a) <= accuracy):\n",
    "        return (b + a) / 2\n",
    "    \n",
    "    c = b - (b - a) / phi\n",
    "    d = a + (b - a) / phi\n",
    "    if f(c) < f(d): \n",
    "        return gss_recursive(f, a, d, accuracy)\n",
    "    else:\n",
    "        return gss_recursive(f, c, b, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply the golden section search to find the minimum of our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Golden Section Search...\n",
      "The minimum of sin(x) over the interval ( 0.0 , 6.283185307179586 ) is at x = 4.712388990891052\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "xleft = 0.\n",
    "xright = 2. * math.pi\n",
    "\n",
    "print(\"Performing Golden Section Search...\")\n",
    "print(\"The minimum of sin(x) over the interval (\",xleft,\",\",xright,\") is at x =\",gss(f,xleft,xright, 1.e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gss](gss.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton method\n",
    "\n",
    "The extremum (minimum or maximum) of a function $f(x)$ is located at a point where its derivative equals zero: $f'(x) = 0$. The Newton-Raphson method can be applied to find these critical points by treating the derivative itself as the function whose root we need to find.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Start with an initial guess $x_0$\n",
    "2. Apply the Newton-Raphson iteration formula for the root of $f(x)$:\n",
    "\n",
    "   $$x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)}$$\n",
    "\n",
    "3. Repeat until convergence (when $|x_{n+1} - x_n|$ is sufficiently small)\n",
    "\n",
    "Technically, the method yields an extremum, which is not necessarily a minimum or maximum. To determine the type of extremum, we can examine the second derivative:\n",
    "- If $f''(x) > 0$, the point is a **minimum**\n",
    "- If $f''(x) < 0$, the point is a **maximum**\n",
    "- If $f''(x) = 0$, the test is inconclusive (need higher-order derivatives)\n",
    "\n",
    "### Advantages and Limitations\n",
    "\n",
    "Advantages:\n",
    "- Quadratic convergence when close to the solution\n",
    "- Highly efficient for well-behaved functions\n",
    "\n",
    "Limitations:\n",
    "- Requires calculation of both first and second derivatives\n",
    "- May diverge if the initial guess is poor\n",
    "- Can fail if $f''(x_n) = 0$ or is very close to zero at any iteration\n",
    "- May converge to a local extremum rather than the global one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation: require the specification of first and second derivatives, the initial guess, and the accuracy of the solution. We also restrict the number of iterations to avoid infinite loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_extremum(df, d2f, x0, accuracy=1e-7, max_iterations=100):\n",
    "    xprev = xnew = x0\n",
    "    for i in range(max_iterations):\n",
    "        xnew = xprev - df(xprev) / d2f(xprev)\n",
    "\n",
    "        if (abs(xnew-xprev) < accuracy):\n",
    "            return xnew\n",
    "    \n",
    "        xprev = xnew\n",
    "    \n",
    "    print(\"Newton method failed to converge to a required precision in \" + str(max_iterations) + \" iterations\")\n",
    "    \n",
    "    return xnew   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the implementation of the Newton method for finding extrema on our example function $f(x) = \\sin(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An extremum of sin(x) using Newton's method starting from x0 =  5.0  is at x = 4.71238898038469\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def df(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "def d2f(x):\n",
    "    return -np.sin(x)\n",
    "\n",
    "x0 = 5.\n",
    "print(\"An extremum of sin(x) using Newton's method starting from x0 = \",x0,\" is at x =\",newton_extremum(df,d2f, x0, 1.e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent \n",
    "\n",
    "### Algorithm\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm for finding a local minimum of a differentiable function. It works by taking steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point.\n",
    "\n",
    "For a one-dimensional function $f(x)$, the gradient descent method can be viewed as a modification of Newton's method where the second derivative is replaced by a descent factor $1/\\gamma_n$:\n",
    "\n",
    "$$x_{n+1} = x_n - \\gamma_n f'(x_n)$$\n",
    "\n",
    "Where:\n",
    "- $\\gamma_n > 0$ for finding a minimum\n",
    "- $\\gamma_n < 0$ for finding a maximum\n",
    "\n",
    "$\\gamma_n$ is called the learning rate or step size\n",
    "\n",
    "### Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(df, x0, gam = 0.01, accuracy=1e-7, max_iterations=100):\n",
    "    xprev  = x0\n",
    "    for i in range(max_iterations):\n",
    "        xnew = xprev - gam * df(xprev)\n",
    "\n",
    "        if (abs(xnew-xprev) < accuracy):\n",
    "            return xnew\n",
    "    \n",
    "        xprev2 = xprev\n",
    "        xprev = xnew\n",
    "    \n",
    "    print(\"Gradient descent method failed to converge to a required precision in \" + str(max_iterations) + \" iterations\")\n",
    "    \n",
    "    return xnew     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it on our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An extremum of sin(x) using gradient descent (gam =  0.1 ) starting from x0 =  5.0  is at x = 4.712397537576874\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def df(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "x0 = 5.\n",
    "gam = 0.1\n",
    "print(\"An extremum of sin(x) using gradient descent (gam = \", gam, \") starting from x0 = \",x0,\" is at x =\",gradient_descent(df,x0, gam, 1.e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "The choice of the learning rate $\\gamma_n$ is critical for the performance of gradient descent. If it is too small, the algorithm will converge slowly, while if it is too large, the algorithm may not converge at all, or get stuck in a loop.\n",
    "\n",
    "It can be worthwhile to have a learning rate schedule, where the learning rate is adjusted during the optimization process, for example, decreasing it over time:\n",
    "\n",
    "$$\\gamma_n = \\frac{\\gamma_0}{1 + a n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional case\n",
    "\n",
    "Gradient descent can be straightforwardly extended to multivariate functions by using the gradient vector instead of the scalar gradient. The update rule becomes:\n",
    "\n",
    "$$\\mathbf{x}_{n+1} = \\mathbf{x}_n - \\gamma_n \\nabla f(\\mathbf{x}_n)$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompPhys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
